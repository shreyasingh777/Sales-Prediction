{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 2015190,
          "sourceType": "datasetVersion",
          "datasetId": 1206038
        }
      ],
      "dockerImageVersionId": 30066,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "Ndd7BOVvliYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Business Problems\n",
        "\n",
        "For example:\n",
        "* How should we allocate our limited marketing budget for next year?\n",
        "* What type on influencers should we focus on?\n",
        "* Can we cut budget from TV, as it is too expensive?\n",
        "* Finally, set your assumptions of the marketing budget and influencer (if any). Can you try to predict the expected sales based on the best ML model?"
      ],
      "metadata": {
        "id": "oGabZHBwliYu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing the Libraries\n",
        "First we import the required libraries:"
      ],
      "metadata": {
        "id": "qe2_rmHhliYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error as mse"
      ],
      "metadata": {
        "trusted": true,
        "id": "T_OavlBAliYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading the Data"
      ],
      "metadata": {
        "id": "p6kPbkUPliYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/kaggle/input/dummy-advertising-and-sales-data/Dummy Data HSS.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "P96JoTHRliYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to look if there's any missing data, as follows:"
      ],
      "metadata": {
        "id": "Do6YTMeIliYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "trusted": true,
        "id": "6imMmlgFliYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see there are mising data in several columns. Thus, we want to fill the missing data with its average, as follows:"
      ],
      "metadata": {
        "id": "oROvoaaRliYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.fillna(df.mean()) # updates the df\n",
        "\n",
        "# see the updated df info\n",
        "\n",
        "df.info()"
      ],
      "metadata": {
        "trusted": true,
        "id": "A37rl9y3liYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have a dataframe that has no missing values. Now, we want to encode the categorical variable to dummy variables, as follows:"
      ],
      "metadata": {
        "id": "lYRQuLvEliY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.get_dummies(df) # updates the df again\n",
        "\n",
        "# see the updated df\n",
        "\n",
        "df"
      ],
      "metadata": {
        "trusted": true,
        "id": "5A1wiLuyliY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now for the purpose of simplicity, we shift the column 'Sales' to the end of the table, as follows:"
      ],
      "metadata": {
        "id": "KahQK1FuliY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns # getting the column names"
      ],
      "metadata": {
        "trusted": true,
        "id": "OLm2pwJuliY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[['TV', 'Radio', 'Social Media', 'Influencer_Macro',\n",
        "       'Influencer_Mega', 'Influencer_Micro', 'Influencer_Nano', 'Sales']]\n",
        "\n",
        "# see the updated df\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "PAoTlZrPliY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Out of curiosity, we just want to explore whether there is any correlation of Sales with its predictors:"
      ],
      "metadata": {
        "id": "c2Khv851liY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.corr()"
      ],
      "metadata": {
        "trusted": true,
        "id": "jpKKdavvliY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have a clean data. Now, we will create the independent and dependent variables (x and y), as follows:"
      ],
      "metadata": {
        "id": "8wXCi9N2liY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = df.iloc[:,0:-1].values\n",
        "y = df.iloc[:,-1:].values"
      ],
      "metadata": {
        "trusted": true,
        "id": "Pbp2F8LOliY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see some preview of x and y:"
      ],
      "metadata": {
        "id": "_0Y_7s3WliY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(x)\n",
        "print(y)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Fmx-OPjnliY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks good. Now, let's split our data for training and testing:"
      ],
      "metadata": {
        "id": "3PIGyVTDliY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x, y)"
      ],
      "metadata": {
        "trusted": true,
        "id": "tFid9AngliY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see some previews:"
      ],
      "metadata": {
        "id": "GZcZ-EwJliY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train)\n",
        "print(y_train)"
      ],
      "metadata": {
        "trusted": true,
        "id": "mAdFD2VOliY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to know the number of data used for training and testing, as follows:"
      ],
      "metadata": {
        "id": "lGVDNuK5liY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(x_train),len(x_test))"
      ],
      "metadata": {
        "trusted": true,
        "id": "AUOosIwcliY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis\n",
        "\n",
        "Now we will train and predict the data based on several regression models:\n",
        "* Linear\n",
        "* Random Forest\n",
        "* Decision Tree\n",
        "* Support Vector\n",
        "* Polynomial\n",
        "\n",
        "For each regression model, we will evaluate its r2_score and root mean squared error (RMSE). The higher r2_score the better; the lower RMSE, the better."
      ],
      "metadata": {
        "id": "6DnjYqW-liY5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Regression"
      ],
      "metadata": {
        "id": "vUQBKyC0liY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr_regressor = LinearRegression() # instantiate the Linear Regression module\n",
        "lr_regressor.fit(x_train, y_train) # training the data\n",
        "\n",
        "# after training the data, perform prediction:\n",
        "\n",
        "y_pred_lr = lr_regressor.predict(x_test) # this is the prediction\n",
        "\n",
        "# evaluate the r2_score and RMSE between prediction and real data\n",
        "\n",
        "print(r2_score(y_test, y_pred_lr))\n",
        "print(mse(y_test, y_pred_lr)**0.5)"
      ],
      "metadata": {
        "trusted": true,
        "id": "C0ftzo0tliY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(lr_regressor.coef_, lr_regressor.intercept_)"
      ],
      "metadata": {
        "trusted": true,
        "id": "1ZO-fp72liY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "trusted": true,
        "id": "w7Vg8sqWliY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression model shows that:\n",
        "\n",
        "**Sales = 3.50 TV + 0.14 Radio + 0.03 Social Media - 0.43 Macro + 0.13 Mega + 0.07 Micro + 0.22 Nano**"
      ],
      "metadata": {
        "id": "XqgX7cRaliY6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest Regression"
      ],
      "metadata": {
        "id": "djx50PbQliY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf_regressor = RandomForestRegressor() # instantiate the Random Forest Regression module\n",
        "rf_regressor.fit(x_train, y_train) # training the data\n",
        "\n",
        "# after training the data, perform prediction:\n",
        "\n",
        "y_pred_rf = rf_regressor.predict(x_test) # prediction data\n",
        "\n",
        "# evaluate the r2_score and RMSE between prediction and real data\n",
        "\n",
        "print(r2_score(y_test, y_pred_rf))\n",
        "print(mse(y_test, y_pred_rf)**0.5)"
      ],
      "metadata": {
        "trusted": true,
        "id": "BD2ihLnGliY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Tree Regression"
      ],
      "metadata": {
        "id": "tesPLmdXliY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Similar processes with the above two examples\n",
        "\n",
        "dt_regressor = DecisionTreeRegressor()\n",
        "dt_regressor.fit(x_train, y_train)\n",
        "\n",
        "y_pred_dt = dt_regressor.predict(x_test)\n",
        "\n",
        "print(r2_score(y_test, y_pred_dt))\n",
        "print(mse(y_test, y_pred_dt)**0.5)"
      ],
      "metadata": {
        "trusted": true,
        "id": "bGhJvp_EliY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Support Vector Regression\n",
        "\n",
        "For SVR, we need to perform feature scaling. In short, this is required because of some mathematical assumptions. So first we will perform feature scaling:"
      ],
      "metadata": {
        "id": "MUUyi-iEliY7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Scaling for Support Vector Regression\n",
        "\n",
        "To avoid confusion of variable names, we will create new independent and dependent variable names: a and b"
      ],
      "metadata": {
        "id": "9l4ayncKliY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = df.iloc[:,0:-1].values\n",
        "b = df.iloc[:,-1:].values\n",
        "\n",
        "# reshape b\n",
        "\n",
        "b = b.reshape(len(b), 1)"
      ],
      "metadata": {
        "trusted": true,
        "id": "dML1Eh_eliZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we split to train and test\n",
        "\n",
        "a_train, a_test, b_train, b_test = train_test_split(a, b)"
      ],
      "metadata": {
        "trusted": true,
        "id": "NbVnrD04liZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now perform scaling\n",
        "\n",
        "scala = StandardScaler()\n",
        "scalb = StandardScaler()\n",
        "\n",
        "a_train = scala.fit_transform(a_train)\n",
        "b_train = scalb.fit_transform(b_train)"
      ],
      "metadata": {
        "trusted": true,
        "id": "7uqglqLLliZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training the SVR\n",
        "\n",
        "sv_regressor = SVR()\n",
        "sv_regressor.fit(a_train, b_train)"
      ],
      "metadata": {
        "trusted": true,
        "id": "hLt06I4hliZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the prediction\n",
        "\n",
        "b_pred = scalb.inverse_transform(sv_regressor.predict(scala.transform(a_test)))"
      ],
      "metadata": {
        "trusted": true,
        "id": "27mN_l0dliZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the model\n",
        "\n",
        "print(r2_score(b_test, b_pred))\n",
        "print(mse(b_test, b_pred)**0.5)"
      ],
      "metadata": {
        "trusted": true,
        "id": "yCMVPZlAliZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Polynomial Regression\n",
        "\n",
        "For Polynomial Regression, we also have to perform polynomial feature scaling."
      ],
      "metadata": {
        "id": "4k-liiKgliZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "poly = PolynomialFeatures()\n",
        "po_regressor = LinearRegression()\n",
        "\n",
        "# training the data\n",
        "\n",
        "po_regressor.fit(poly.fit_transform(x_train), y_train)"
      ],
      "metadata": {
        "trusted": true,
        "id": "8O_xvv7YliZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prediction training\n",
        "\n",
        "y_pred_po = po_regressor.predict(poly.fit_transform(x_test))\n",
        "\n",
        "# evaluate the model\n",
        "\n",
        "print(r2_score(y_test, y_pred_po))\n",
        "print(mse(y_test, y_pred_po)**0.5)"
      ],
      "metadata": {
        "trusted": true,
        "id": "SFWyYkuJliZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation\n",
        "\n",
        "* Based on our analysis, it can be inferred that Support Vector Regression yields the highest R2 score and lowest RMSE. Thus, SVR will be the chosen model. We can use SVR model to predict the sales based on the input strategy.\n",
        "* Based on linear regression, we can infer that we can focus on TV, Radio, and Nano Influencer. We can cut budget related to other factors."
      ],
      "metadata": {
        "id": "aqY8Q4tFliZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Concluding Remarks\n",
        "\n",
        "* I want this notebook to be as understandable as possible so students or people like me who are not from math/data science/computer science background can infer easily what is happening in each line of code.\n",
        "* I understand there will be several math/ML/any other assumptions that I might have skipped in this notebook (e.g. assumptions for Linear Regression, feature scaling, parameter tuning). Please let me know in the comments to improve it.\n",
        "* Overall, this is part of my learning journey as well as a business management scholar. So, I will just focus on the 'broad overview' of the algorithms, and shed a light more on *making sense* of how data can be used for better decision making\n",
        "\n",
        "You can upvote if you like this notebook. Happy (machine) learning!"
      ],
      "metadata": {
        "id": "8vFTh-YoliZS"
      }
    }
  ]
}